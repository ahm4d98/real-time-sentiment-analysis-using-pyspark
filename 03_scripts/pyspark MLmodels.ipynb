{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import FMClassifier\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting(data):\n",
    "   # data=data.limit(10000)\n",
    "    data=data.replace(4,1)\n",
    "    dividedData = data.randomSplit([0.7, 0.3]) \n",
    "    trainingData = dividedData[0] #index 0 = data training\n",
    "    testingData = dividedData[1] #index 1 = data testing\n",
    "    train_rows = trainingData.count()\n",
    "    test_rows = testingData.count()\n",
    "    print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)\n",
    "    return trainingData,testingData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(datadir):\n",
    "    #reading data from directory \n",
    "    tweets=spark.read.csv(datadir,inferSchema=True, header=False)\n",
    "    #setting meaningful column name\n",
    "    tweets.createOrReplaceTempView(\"tweets\")\n",
    "    tweets=spark.sql(\"select _c0 as label, _c1 as ID, _c2 as value ,_c3 as flag ,_c4 as user,_c5 as SentimentText from tweets\")\n",
    "    #selecting relevent columns   \n",
    "    data = tweets.select(\"SentimentText\", 'Label')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(input_col_tockenizer,output_col_tockenizer,stop_word_column,tokenizedTrain):\n",
    "    tokenizer = Tokenizer(inputCol=input_col_tockenizer,outputCol=output_col_tockenizer)\n",
    "    tokenizedTrain = tokenizer.transform(tokenizedTrain)\n",
    "    stopwords_remover = StopWordsRemover(inputCol=output_col_tockenizer,outputCol=stop_word_column)\n",
    "    SwRemovedTrain = stopwords_remover.transform(tokenizedTrain)\n",
    "    hashTF = HashingTF(inputCol=stopwords_remover.getOutputCol(), outputCol=\"features\")\n",
    "    numericTrainData = hashTF.transform(SwRemovedTrain).select('label', 'MeaningfulWords', 'features')\n",
    "\n",
    "    return numericTrainData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_naivebayes_model():\n",
    "    nb = NaiveBayes(modelType=\"multinomial\")\n",
    "    nbparamGrid = (ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build())\n",
    "\n",
    "    eval_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")\n",
    "    nbcv = CrossValidator(estimator = nb,estimatorParamMaps = nbparamGrid,\n",
    "                        evaluator = eval_auc,\n",
    "                        numFolds = 5)\n",
    "    return nbcv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient booster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosted_classifier(maxIter):\n",
    "    gbtr = GBTRegressor(featuresCol='features', labelCol='label', maxIter=maxIter)\n",
    "    return gbtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisitic_regression():\n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "    grid = ParamGridBuilder().addGrid(lr.maxIter, [1,20,30,1000]) \\\n",
    "                                .addGrid(lr.regParam, [0.1,0.01]) \\\n",
    "                                .addGrid(lr.elasticNetParam, [1]) \\\n",
    "                                .build()\n",
    "    lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, \\\n",
    "                            evaluator=eval_auc, numFolds=10)\n",
    "    return lr_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fm classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmclassifier():\n",
    "    fm = FMClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    grid = ParamGridBuilder().addGrid(fm.maxIter, [1,20,30]) \\\n",
    "                                .addGrid(fm.regParam, [0.1,0.01]) \\\n",
    "                                 \\\n",
    "                                .build()\n",
    "    fm_class = CrossValidator(estimator=fm, estimatorParamMaps=grid, \\\n",
    "                            evaluator=eval_auc, numFolds=10)\n",
    "\n",
    "    return fm_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_testing(data):\n",
    "    data.createOrReplaceTempView(\"tweets\")\n",
    "    tweets=spark.sql(\"select _c0 as Label, _c1 as SentimentText from tweets\")\n",
    "    #selecting relevent columns   \n",
    "    data = tweets.select(\"SentimentText\", 'Label')\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .appName('my-cool-app') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=r'C:\\Users\\ahmed hatem\\Downloads\\archive (2)\\training.1600000.processed.noemoticon.csv'\n",
    "data=data_loading(datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_preprocessing('SentimentText','SentimentWords','MeaningfulWords',data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1119652 ; Testing data rows: 480348\n"
     ]
    }
   ],
   "source": [
    "train,test=data_splitting(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine elarning models used for prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbcv=best_naivebayes_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  gradient boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC=gradient_boosted_classifier(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logisitc regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model=logisitic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fmclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm=fmclassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " naive bayes training is done\n"
     ]
    }
   ],
   "source": [
    "nbcvModel = nbcv.fit(train)\n",
    "print(' naive bayes training is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " naive bayes training is done\n"
     ]
    }
   ],
   "source": [
    "lrModel = logistic_model.fit(train)\n",
    "print(' naive bayes training is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GBC trianing is done\n"
     ]
    }
   ],
   "source": [
    "gbtr = GBC.fit(train)\n",
    "print(' GBC trianing is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FM training is done\n"
     ]
    }
   ],
   "source": [
    "FM = fm.fit(train)\n",
    "print(' FM training is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = nbcvModel.bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisitc = lrModel.bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = gbtr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_predict = FM.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_prediction = eval_auc.evaluate(naive_bayes)\n",
    "nb_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisitc_predict = eval_auc.evaluate(logisitc)\n",
    "logisitc_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_predict = eval_auc.evaluate(gbt)\n",
    "gbt_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predict = eval_auc.evaluate(FM_predict)\n",
    "fm_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on live dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_tweets=spark.read.csv(r'C:\\Users\\ahmed hatem\\Downloads\\live twiiter data\\live tweets for testing_cleaned.csv',inferSchema=True,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_tweets=live_tweets.na.drop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_data=preprocessing_for_testing(live_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_testing=data_preprocessing('SentimentText','SentimentWords','MeaningfulWords',live_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on live dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_prediction=nbcvModel.bestModel.transform(data_testing.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisitic_regression_predictions = lrModel.bestModel.transform(data_testing.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBTR_predictions = gbtr.transform(data_testing.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictions = FM.transform(data_testing.select('features'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
